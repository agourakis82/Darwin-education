#!/usr/bin/env python3
"""Create placeholder benchmark artifacts when full runtime is blocked."""

from __future__ import annotations

import json
import statistics
import time
from datetime import datetime, timezone
from pathlib import Path


def percentile(values, q):
    if not values:
        return None
    s = sorted(values)
    k = (len(s) - 1) * q
    f = int(k)
    c = min(f + 1, len(s) - 1)
    if f == c:
        return s[f]
    d0 = s[f] * (c - k)
    d1 = s[c] * (k - f)
    return d0 + d1


def main() -> None:
    repo_root = Path(__file__).resolve().parents[2]
    pack_dir = repo_root / "_paperpack"
    pack_dir.mkdir(parents=True, exist_ok=True)

    # Deterministic synthetic timings (ms) for placeholder reporting.
    synthetic_ms = [122, 118, 130, 125, 121, 127, 119, 124, 129, 123]

    result = {
        "generated_at_utc": datetime.now(timezone.utc).strftime("%Y-%m-%dT%H:%M:%SZ"),
        "mode": "mock",
        "n": len(synthetic_ms),
        "latency_ms": {
            "p50": percentile(synthetic_ms, 0.50),
            "p95": percentile(synthetic_ms, 0.95),
            "mean": statistics.mean(synthetic_ms),
            "min": min(synthetic_ms),
            "max": max(synthetic_ms),
        },
        "throughput_items_per_s": round(1000.0 / statistics.mean(synthetic_ms), 4),
        "failure_rate": 0.0,
        "token_cost_proxy": "NOT FOUND",
        "note": "Mock-only placeholder metrics; replace with real pipeline execution when dependencies are available.",
    }

    json_out = pack_dir / "mock_benchmark_results.json"
    md_out = pack_dir / "mock_benchmark_results.md"

    json_out.write_text(json.dumps(result, indent=2, ensure_ascii=True) + "\n", encoding="utf-8")

    md_lines = [
        "# Mock Benchmark Results",
        "",
        "This file is generated by `_paperpack/scripts/mock_benchmark_driver.py` as a placeholder only.",
        "",
        f"- n: {result['n']}",
        f"- p50 latency (ms): {result['latency_ms']['p50']}",
        f"- p95 latency (ms): {result['latency_ms']['p95']}",
        f"- mean latency (ms): {result['latency_ms']['mean']:.2f}",
        f"- throughput (items/s): {result['throughput_items_per_s']}",
        f"- failure rate: {result['failure_rate']}",
        f"- token cost proxy: {result['token_cost_proxy']}",
        f"- note: {result['note']}",
    ]
    md_out.write_text("\n".join(md_lines).rstrip() + "\n", encoding="utf-8")


if __name__ == "__main__":
    main()
